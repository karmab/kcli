#!/usr/bin/env python

from base64 import b64encode, b64decode
from glob import glob
import json
import os
import sys
from ipaddress import ip_network
from kvirt.common import error, pprint, success, warning, info2
from kvirt.common import get_oc, pwd_path, get_oc_mirror
from kvirt.common import get_latest_fcos, generate_rhcos_iso, olm_app, get_commit_rhcos
from kvirt.common import get_installer_rhcos, wait_cloud_dns, delete_lastvm
from kvirt.common import ssh, scp, _ssh_credentials, get_ssh_pub_key, boot_baremetal_hosts, separate_yamls
from kvirt.defaults import LOCAL_OPENSHIFT_APPS, OPENSHIFT_TAG
import re
from random import choice
from shutil import copy2, move, rmtree, which
from socket import gethostbyname
from string import ascii_letters, digits
from subprocess import call
from tempfile import TemporaryDirectory
from time import sleep
from urllib.request import urlopen, Request
import yaml


virtplatforms = ['kvm', 'kubevirt', 'ovirt', 'openstack', 'vsphere', 'proxmox']
cloudplatforms = ['aws', 'azure', 'gcp', 'ibm']


def aws_credentials(config):
    if os.path.exists(os.path.expanduser('~/.aws/credentials')):
        return
    aws_dir = f'{os.environ["HOME"]}/.aws'
    if not os.path.exists(aws_dir):
        os.mkdir(aws_dir)
    access_key_id = config.options.get('access_key_id')
    access_key_secret = config.options.get('access_key_secret')
    session_token = config.options.get('session_token')
    with open(f"{aws_dir}/credentials", "w") as f:
        data = """[default]
aws_access_key_id={access_key_id}
aws_secret_access_key={access_key_secret}""".format(access_key_id=access_key_id, access_key_secret=access_key_secret)
        f.write(data)
        if session_token is not None:
            f.write(f"aws_session_token={session_token}")


def mapping_to_icsp(config, plandir, output_dir, mirror_config):
    mapping_file = glob("oc-mirror-workspace/results-*/mapping.txt")[0]
    mirrors = []
    index_images = []
    # Existing catalog index images are gathered from oc-mirror output
    # if the index is generated by oc-mirror we don't want to regenerate it here
    existing_catalog_index_images = []
    for catalogsource in glob("oc-mirror-workspace/results-*/catalogSource*.yaml"):
        catalog_data = yaml.safe_load(open(catalogsource, 'r'))
        catalog_index_image = catalog_data['spec']['image']
        existing_catalog_index_images.append(catalog_index_image)

    for line in open(mapping_file, 'r').readlines():
        result = re.search(r"(.+)=(.+)", line)
        source_registry_image = result.group(1)
        mirror_registry_image = result.group(2)
        # If we find some index image, we add it to this list to generate a catalogsource for it later
        if "index:v" in mirror_registry_image and mirror_registry_image not in existing_catalog_index_images:
            index_images.append(mirror_registry_image)
        result = re.search(r"(.+)/", source_registry_image)
        source_registry_namespace = result.group(1)
        result = re.search(r"(.+)/", mirror_registry_image)
        mirror_registry_namespace = result.group(1)
        icsp_entry = {"source_registry_namespace": source_registry_namespace,
                      "mirror_registry_namespace": mirror_registry_namespace}
        mirrors.append(icsp_entry)
    mirror_list = [dict(t) for t in {tuple(d.items()) for d in mirrors}]
    if mirror_config is not None:
        config_data = yaml.safe_load(open(mirror_config, 'r'))
        mirror_registry = config_data['storageConfig']['registry']['imageURL'].split("/")[0]
        for catalog in config_data['mirror']['operators']:
            image_namespace = catalog['catalog'].split("/")[1]
            image_name = catalog['catalog'].split("/")[2]
            index_image = f"{mirror_registry}/{image_namespace}/{image_name}"
            if index_image not in existing_catalog_index_images:
                index_images.append(index_image)
    # Remove duplicates from index images
    index_images = list(dict.fromkeys(index_images))
    if len(index_images) > 0:
        catalogsource = config.process_inputfile('xxx', f"{plandir}/catalogsource.yml.j2",
                                                 overrides={'index_images': index_images})
        with open(f"{output_dir}/catalogSource.yaml", 'w') as f:
            f.write(catalogsource)
    # Create ImageContentSourcePolicy file
    icsp = config.process_inputfile('xxx', f"{plandir}/icsp.yml.j2", overrides={'mirror_list': mirror_list})
    with open(f"{output_dir}/imageContentSourcePolicy.yaml", 'w') as f:
        f.write(icsp)


def update_pull_secret(pull_secret, registry, user, password):
    pull_secret = os.path.expanduser(pull_secret)
    data = json.load(open(pull_secret))
    auths = data['auths']
    if registry not in auths or b64decode(auths[registry]['auth']).decode("utf-8").split(':') != [user, password]:
        pprint(f"Updating your pull secret with entry for {registry}")
        key = f"{user}:{password}"
        key = str(b64encode(key.encode('utf-8')), 'utf-8')
        data['auths'][registry] = {'auth': key, 'email': 'jhendrix@karmalabs.corp'}
        with open(pull_secret, 'w') as p:
            json.dump(data, p)


def create_ignition_files(config, plandir, cluster, domain, api_ip=None, bucket_url=None, ignition_version=None):
    clusterdir = os.path.expanduser(f"~/.kcli/clusters/{cluster}")
    ignition_overrides = {'api_ip': api_ip, 'cluster': cluster, 'domain': domain, 'role': 'master'}
    ctlplane_ignition = config.process_inputfile(cluster, f"{plandir}/ignition.j2", overrides=ignition_overrides)
    with open(f"{clusterdir}/ctlplane.ign", 'w') as f:
        f.write(ctlplane_ignition)
    del ignition_overrides['role']
    worker_ignition = config.process_inputfile(cluster, f"{plandir}/ignition.j2", overrides=ignition_overrides)
    with open(f"{clusterdir}/worker.ign", 'w') as f:
        f.write(worker_ignition)
    if bucket_url is not None:
        ignition_overrides['bucket_url'] = bucket_url
        bootstrap_ignition = config.process_inputfile(cluster, f"{plandir}/ignition.j2", overrides=ignition_overrides)
        with open(f"{clusterdir}/bootstrap.ign", 'w') as f:
            f.write(bootstrap_ignition)


def backup_paramfile(installparam, clusterdir, cluster, plan, image, dnsconfig):
    with open(f"{clusterdir}/kcli_parameters.yml", 'w') as p:
        installparam['cluster'] = cluster
        installparam['plan'] = plan
        installparam['image'] = image
        if dnsconfig is not None:
            installparam['dnsclient'] = dnsconfig.client
        yaml.safe_dump(installparam, p, default_flow_style=False, encoding='utf-8', allow_unicode=True)


def update_openshift_etc_hosts(cluster, domain, host_ip, ingress_ip=None):
    if not os.path.exists("/i_am_a_container"):
        hosts = open("/etc/hosts").readlines()
        wronglines = [e for e in hosts if not e.startswith('#') and f"api.{cluster}.{domain}" in e and
                      host_ip not in e]
        if ingress_ip is not None:
            o = f"oauth-openshift.apps.{cluster}.{domain}"
            wrongingresses = [e for e in hosts if not e.startswith('#') and o in e and ingress_ip not in e]
            wronglines.extend(wrongingresses)
        for wrong in wronglines:
            warning(f"Cleaning wrong entry {wrong} in /etc/hosts")
            call(f"sudo sed -i '/{wrong.strip()}/d' /etc/hosts", shell=True)
        hosts = open("/etc/hosts").readlines()
        correct = [e for e in hosts if not e.startswith('#') and f"api.{cluster}.{domain}" in e and host_ip in e]
        if not correct:
            entries = [f"api.{cluster}.{domain}"]
            ingress_entries = [f"{x}.{cluster}.{domain}" for x in ['console-openshift-console.apps',
                               'oauth-openshift.apps', 'prometheus-k8s-openshift-monitoring.apps']]
            if ingress_ip is None:
                entries.extend(ingress_entries)
            entries = ' '.join(entries)
            call(f"sudo sh -c 'echo {host_ip} {entries} >> /etc/hosts'", shell=True)
            if ingress_ip is not None:
                entries = ' '.join(ingress_entries)
                call(f"sudo sh -c 'echo {ingress_ip} {entries} >> /etc/hosts'", shell=True)
    else:
        entries = [f"api.{cluster}.{domain}"]
        ingress_entries = [f"{x}.{cluster}.{domain}" for x in ['console-openshift-console.apps',
                                                               'oauth-openshift.apps',
                                                               'prometheus-k8s-openshift-monitoring.apps']]
        if ingress_ip is None:
            entries.extend(ingress_entries)
        entries = ' '.join(entries)
        call(f"sh -c 'echo {host_ip} {entries} >> /etc/hosts'", shell=True)
        if os.path.exists('/etcdir/hosts'):
            call(f"sh -c 'echo {host_ip} {entries} >> /etcdir/hosts'", shell=True)
            if ingress_ip is not None:
                entries = ' '.join(ingress_entries)
                call(f"sh -c 'echo {ingress_ip} {entries} >> /etcdir/hosts'", shell=True)
        else:
            warning("Make sure to have the following entry in your /etc/hosts")
            warning(f"{host_ip} {entries}")


def get_installer_version():
    installer_version = os.popen('openshift-install version').readlines()[0].split(" ")[1].strip()
    if installer_version.startswith('v'):
        installer_version = installer_version[1:]
    return installer_version


def offline_image(version='stable', tag='4.13', pull_secret='openshift_pull.json'):
    tag = str(tag).split(':')[-1].split('-')[0]
    offline = 'xxx'
    if version in ['ci', 'nightly']:
        if version == "nightly":
            nightly_url = f"https://amd64.ocp.releases.ci.openshift.org/api/v1/releasestream/{tag}.0-0.nightly/latest"
            tag = json.loads(urlopen(nightly_url).read())['pullSpec']
        cmd = f"oc adm release info registry.ci.openshift.org/ocp/release:{tag} -a {pull_secret}"
        for line in os.popen(cmd).readlines():
            if 'Pull From: ' in str(line):
                offline = line.replace('Pull From: ', '').strip()
                break
        return offline
    ocp_repo = 'ocp-dev-preview' if version == 'dev-preview' else 'ocp'
    if version in ['dev-preview', 'stable']:
        target = tag if len(str(tag).split('.')) > 2 else f'latest-{tag}'
        url = f"https://mirror.openshift.com/pub/openshift-v4/clients/{ocp_repo}/{target}/release.txt"
    elif version == 'latest':
        url = f"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/{version}-{tag}/release.txt"
    try:
        lines = urlopen(url).readlines()
        for line in lines:
            if 'Pull From: ' in str(line):
                offline = line.decode("utf-8").replace('Pull From: ', '').strip()
                break
    except Exception as e:
        error(f"Hit {e} when opening {url}")
    return offline


def same_release_images(version='stable', tag='4.13', pull_secret='openshift_pull.json', path='.'):
    if not os.path.exists(f'{path}/openshift-install'):
        return False
    try:
        existing = os.popen(f'{path}/openshift-install version').readlines()[2].split(" ")[2].strip()
    except:
        return False
    if os.path.abspath(path) != os.getcwd() and not existing.startswith('quay.io/openshift-release-dev/ocp-release')\
       and not existing.startswith('registry.ci.openshift.org/ocp/release'):
        warning("Assuming your disconnected openshift-install has the correct version")
        return True
    offline = offline_image(version=version, tag=tag, pull_secret=pull_secret)
    return offline == existing


def get_installer_minor(installer_version):
    return int(installer_version.split('.')[1])


def get_release_image():
    release_image = os.popen('openshift-install version').readlines()[2].split(" ")[2].strip()
    return release_image


def get_downstream_installer(devpreview=False, macosx=False, tag=None, debug=False, pull_secret='openshift_pull.json'):
    arch = 'arm64' if os.uname().machine == 'aarch64' else None
    repo = 'ocp-dev-preview' if devpreview else 'ocp'
    if tag is None:
        repo += '/latest'
    elif str(tag).count('.') == 1:
        repo += f'/latest-{tag}'
    else:
        repo += '/%s' % tag.replace('-x86_64', '')
    INSTALLSYSTEM = 'mac' if os.path.exists('/Users') or macosx else 'linux'
    url = f"https://mirror.openshift.com/pub/openshift-v4/clients/{repo}"
    msg = f'Downloading openshift-install from {url}'
    pprint(msg)
    try:
        r = urlopen(f"{url}/release.txt").readlines()
    except:
        error(f"Couldn't open url {url}")
        return 1
    version = None
    for line in r:
        if 'Name' in str(line):
            version = str(line).split(':')[1].strip().replace('\\n', '').replace("'", "")
            break
    if version is None:
        error("Couldn't find version")
        return 1
    if arch == 'arm64':
        cmd = f"curl -s https://mirror.openshift.com/pub/openshift-v4/{arch}/clients/{repo}/"
    else:
        cmd = f"curl -s https://mirror.openshift.com/pub/openshift-v4/clients/{repo}/"
    cmd += f"openshift-install-{INSTALLSYSTEM}-{version}.tar.gz "
    cmd += "| tar zxf - openshift-install"
    cmd += "; chmod 700 openshift-install"
    if debug:
        pprint(cmd)
    return call(cmd, shell=True)


def get_upstream_installer(tag, version='stable', debug=False):
    if 'quay.io' not in tag and 'registry.ci.openshift.org' not in tag:
        if version == 'dev-preview':
            url = "https://amd64.origin.releases.ci.openshift.org/api/v1/releasestream/4-scos-next/latest"
        elif version in ['ci', 'nightly']:
            url = f"https://amd64.origin.releases.ci.openshift.org/api/v1/releasestream/{tag}.0-0.okd/latest"
        elif version == 'scos':
            url = "https://amd64.origin.releases.ci.openshift.org/api/v1/releasestream/4-scos-stable/latest"
        else:
            url = "https://amd64.origin.releases.ci.openshift.org/api/v1/releasestream/4-stable/latest"
        tag = json.loads(urlopen(url).read())['pullSpec']
    cmd = f"oc adm release extract --command=openshift-install --to . {tag}"
    cmd += "; chmod 700 openshift-install"
    msg = f'Downloading openshift-install {tag} in current directory'
    pprint(msg)
    if debug:
        pprint(cmd)
    return call(cmd, shell=True)


def get_ci_installer(pull_secret, tag=None, macosx=False, debug=False, nightly=False):
    arch = 'arm64' if os.uname().machine == 'aarch64' else None
    base = 'openshift'
    if 'registry.ci.openshift.org' not in open(os.path.expanduser(pull_secret)).read():
        error("entry for registry.ci.openshift.org missing in pull secret")
        return 1
    if tag is not None and nightly:
        nightly_url = f"https://amd64.ocp.releases.ci.openshift.org/api/v1/releasestream/{tag}.0-0.nightly/latest"
        tag = json.loads(urlopen(nightly_url).read())['pullSpec']
    if tag is None:
        tags = []
        r = urlopen(f"https://{base}-release.ci.openshift.org/graph?format=dot").readlines()
        for line in r:
            tag_match = re.match('.*label="(.*.)", shape=.*', str(line))
            if tag_match is not None:
                tags.append(tag_match.group(1))
        tag = sorted(tags)[-1]
    elif str(tag).startswith('ci-ln'):
        tag = f'registry.build01.ci.openshift.org/{tag}'
    elif '/' not in str(tag):
        if arch == 'arm64':
            tag = f'registry.ci.openshift.org/ocp-arm64/release-arm64:{tag}'
        else:
            basetag = 'ocp'
            tag = f'registry.ci.openshift.org/{basetag}/release:{tag}'
    os.environ['OPENSHIFT_RELEASE_IMAGE'] = tag
    msg = f'Downloading openshift-install {tag} in current directory'
    pprint(msg)
    cmd = f"oc adm release extract --registry-config {pull_secret} --command=openshift-install --to . {tag}"
    cmd += "; chmod 700 openshift-install"
    if debug:
        pprint(cmd)
    return call(cmd, shell=True)


def process_apps(config, clusterdir, apps, overrides):
    if not apps:
        return
    os.environ['KUBECONFIG'] = f"{clusterdir}/auth/kubeconfig"
    for app in apps:
        base_data = overrides.copy()
        if isinstance(app, str):
            appname = app
        elif isinstance(app, dict):
            appname = app.get('name')
            if appname is None:
                error(f"Missing name in dict {app}. Skipping")
                continue
            base_data.update(app)
        if 'apps_install_cr' in base_data:
            base_data['install_cr'] = base_data['apps_install_cr']
        if appname in LOCAL_OPENSHIFT_APPS:
            name = appname
            app_data = base_data
        else:
            name, source, channel, csv, description, namespace, channels, crd = olm_app(appname)
            if name is None:
                error(f"Couldn't find any app matching {app}. Skipping...")
                continue
            app_data = {'name': name, 'source': source, 'channel': channel, 'namespace': namespace, 'crd': crd}
            app_data.update(base_data)
        pprint(f"Adding app {name}")
        config.create_app_openshift(name, app_data)


def process_postscripts(clusterdir, postscripts):
    if not postscripts:
        return
    os.environ['KUBECONFIG'] = f"{clusterdir}/auth/kubeconfig"
    currentdir = pwd_path(".")
    for script in postscripts:
        script_path = os.path.expanduser(script) if script.startswith('/') else f'{currentdir}/{script}'
        pprint(f"Running script {os.path.basename(script)}")
        call(script_path, shell=True)


def wait_for_ignition(cluster, domain, role='worker'):
    clusterdir = os.path.expanduser(f"~/.kcli/clusters/{cluster}")
    ignitionfile = f"{clusterdir}/ctlplane.ign" if role == 'master' else f"{clusterdir}/worker.ign"
    os.remove(ignitionfile)
    while not os.path.exists(ignitionfile) or os.stat(ignitionfile).st_size == 0:
        try:
            with open(ignitionfile, 'w') as dest:
                req = Request(f"http://api.{cluster}.{domain}:22624/config/{role}")
                req.add_header("Accept", "application/vnd.coreos.ignition+json; version=3.1.0")
                data = urlopen(req).read()
                dest.write(data.decode("utf-8"))
        except:
            pprint(f"Waiting 10s before retrieving {role} ignition data")
            sleep(10)


def handle_baremetal_iso(config, plandir, cluster, overrides, baremetal_hosts=[], iso_pool=None):
    baremetal_iso_overrides = overrides.copy()
    baremetal_iso_overrides['noname'] = True
    baremetal_iso_overrides['workers'] = 1
    baremetal_iso_overrides['role'] = 'worker'
    result = config.plan(cluster, inputfile=f'{plandir}/workers.yml', overrides=baremetal_iso_overrides,
                         onlyassets=True)
    iso_data = result['assets'][0]
    ignitionfile = f'{cluster}-worker'
    with open(ignitionfile, 'w') as f:
        f.write(iso_data)
    config.create_openshift_iso(cluster, overrides=baremetal_iso_overrides, ignitionfile=ignitionfile, podman=True,
                                installer=True)
    os.remove(ignitionfile)
    if baremetal_hosts:
        iso_pool_path = config.k.get_pool_path(iso_pool)
        chmodcmd = f"chmod 666 {iso_pool_path}/{cluster}-worker.iso"
        call(chmodcmd, shell=True)
        pprint("Creating httpd deployment to host iso for baremetal workers")
        timeout = 0
        while True:
            if os.popen('oc -n default get pod -l app=httpd-kcli -o name').read() != "":
                break
            if timeout > 60:
                error("Timeout waiting for httpd deployment to be up")
                sys.exit(1)
            httpdcmd = f"oc create -f {plandir}/httpd.yaml"
            call(httpdcmd, shell=True)
            timeout += 5
            sleep(5)
        svcip_cmd = 'oc get node -o yaml'
        svcip = yaml.safe_load(os.popen(svcip_cmd).read())['items'][0]['status']['addresses'][0]['address']
        svcport_cmd = 'oc get svc -n default httpd-kcli-svc -o yaml'
        svcport = yaml.safe_load(os.popen(svcport_cmd).read())['spec']['ports'][0]['nodePort']
        podname = os.popen('oc -n default get pod -l app=httpd-kcli -o name').read().split('/')[1].strip()
        try:
            call(f"oc wait -n default --for=condition=Ready pod/{podname}", shell=True)
        except Exception as e:
            error(f"Hit {e}")
            sys.exit(1)
        copycmd = f"oc -n default cp {iso_pool_path}/{cluster}-worker.iso {podname}:/var/www/html"
        call(copycmd, shell=True)
        return f'http://{svcip}:{svcport}/{cluster}-worker.iso'


def handle_baremetal_iso_sno(config, plandir, cluster, data, baremetal_hosts=[], iso_pool=None):
    iso_name = f"{cluster}-sno.iso"
    baremetal_web = data.get('baremetal_web', True)
    baremetal_web_dir = data.get('baremetal_web_dir', '/var/www/html')
    baremetal_web_subdir = data.get('baremetal_web_subdir')
    if baremetal_web_subdir is not None:
        baremetal_web_dir += f'/{baremetal_web_subdir}'
    baremetal_web_port = data.get('baremetal_web_port', 80)
    iso_pool_path = config.k.get_pool_path(iso_pool)
    if baremetal_web:
        if os.path.exists(f"{baremetal_web_dir}/{iso_name}"):
            call(f"sudo rm {baremetal_web_dir}/{iso_name}", shell=True)
            call(f"sudo rm {baremetal_web_dir}/{iso_name}", shell=True)
        call(f'sudo cp {iso_pool_path}/{iso_name} {baremetal_web_dir}/{iso_name}', shell=True)
        if baremetal_web_dir == '/var/www/html':
            call(f"sudo chown apache:apache {baremetal_web_dir}/{iso_name}", shell=True)
            if which('getenforce') is not None and os.popen('getenforce').read().strip() == 'Enforcing':
                call(f"sudo restorecon -Frvv {baremetal_web_dir}/{iso_name}", shell=True)
    else:
        call(f"sudo chmod a+r {iso_pool_path}/{iso_name}", shell=True)
    nic = os.popen('ip r | grep default | cut -d" " -f5 | head -1').read().strip()
    ip_cmd = f"ip -o addr show {nic} | awk '{{print $4}}' | cut -d '/' -f 1 | head -1"
    host_ip = os.popen(ip_cmd).read().strip()
    if baremetal_web_port != 80:
        host_ip += f":{baremetal_web_port}"
    if baremetal_web_subdir is not None:
        iso_name = f'{baremetal_web_subdir}/{iso_name}'
    return f'http://{host_ip}/{iso_name}'


def scale(config, plandir, cluster, overrides):
    plan = cluster
    client = config.client
    platform = config.type
    k = config.k
    data = {}
    installparam = {}
    pprint(f"Scaling on client {client}")
    clusterdir = os.path.expanduser(f"~/.kcli/clusters/{cluster}")
    if not os.path.exists(clusterdir):
        warning(f"Creating {clusterdir} from your input (auth creds will be missing)")
        overrides['cluster'] = cluster
        api_ip = overrides.get('api_ip')
        if config.type not in cloudplatforms and api_ip is None:
            msg = 'Missing api_ip...'
            return {'result': 'failure', 'reason': msg}
        domain = overrides.get('domain')
        if domain is None:
            msg = "Missing domain..."
            return {'result': 'failure', 'reason': msg}
        os.mkdir(clusterdir)
        ignition_version = overrides['ignition_version']
        create_ignition_files(config, plandir, cluster, domain, api_ip=api_ip, ignition_version=ignition_version)
    if os.path.exists(f"{clusterdir}/kcli_parameters.yml"):
        with open(f"{clusterdir}/kcli_parameters.yml", 'r') as install:
            installparam = yaml.safe_load(install)
            data.update(installparam)
            plan = installparam.get('plan', plan)
    data.update(overrides)
    if os.path.exists(clusterdir):
        with open(f"{clusterdir}/kcli_parameters.yml", 'w') as paramfile:
            yaml.safe_dump(data, paramfile)
    image = data.get('image')
    if image is None:
        cluster_image = k.info(f"{cluster}-ctlplane-0").get('image')
        if cluster_image is None:
            msg = "Missing image..."
            return {'result': 'failure', 'reason': msg}
        else:
            pprint(f"Using image {cluster_image}")
            image = cluster_image
    data['image'] = image
    old_baremetal_hosts = installparam.get('baremetal_hosts', [])
    new_baremetal_hosts = overrides.get('baremetal_hosts', [])
    baremetal_hosts = [entry for entry in new_baremetal_hosts if entry not in old_baremetal_hosts]
    if baremetal_hosts:
        if not old_baremetal_hosts:
            iso_pool = data.get('pool') or config.pool
            iso_url = handle_baremetal_iso(config, plandir, cluster, data, baremetal_hosts, iso_pool)
        else:
            svcip_cmd = 'oc get node -o yaml'
            svcip = yaml.safe_load(os.popen(svcip_cmd).read())['items'][0]['status']['addresses'][0]['address']
            svcport_cmd = 'oc get svc -n default httpd-kcli-svc -o yaml'
            svcport = yaml.safe_load(os.popen(svcport_cmd).read())['spec']['ports'][0]['nodePort']
            iso_url = f'http://{svcip}:{svcport}/{cluster}-worker.iso'
        result = boot_baremetal_hosts(baremetal_hosts, iso_url, overrides=overrides, debug=config.debug)
        if result['result'] != 'success':
            return result
        overrides['workers'] = overrides.get('workers', 0) - len(new_baremetal_hosts)
    for role in ['ctlplanes', 'workers']:
        overrides = data.copy()
        threaded = data.get('threaded', False) or data.get(f'{role}_threaded', False)
        if overrides.get(role, 0) <= 0:
            continue
        if platform in virtplatforms:
            os.chdir(os.path.expanduser("~/.kcli"))
            if role == 'ctlplanes' and ('virtual_router_id' not in overrides or 'auth_pass' not in overrides):
                warning("Scaling up of ctlplanes won't work without virtual_router_id and auth_pass")
            result = config.plan(plan, inputfile=f'{plandir}/{role}.yml', overrides=overrides, threaded=threaded)
        elif platform in cloudplatforms:
            result = config.plan(plan, inputfile=f'{plandir}/cloud_{role}.yml', overrides=overrides, threaded=threaded)
        if result['result'] != 'success':
            return result
    return {'result': 'success'}


def create(config, plandir, cluster, overrides, dnsconfig=None):
    k = config.k
    log_level = 'debug' if config.debug else 'info'
    client = config.client
    platform = config.type
    arch = k.get_capabilities()['arch'] if platform == 'kvm' else 'x86_64'
    pprint(f"Deploying on client {client}")
    data = {'domain': 'karmalabs.corp',
            'network': 'default',
            'ctlplanes': 3,
            'workers': 0,
            'tag': OPENSHIFT_TAG,
            'ipv6': False,
            'pull_secret': 'openshift_pull.json',
            'version': 'stable',
            'macosx': False,
            'fips': False,
            'apps': [],
            'dualstack': False,
            'kvm_forcestack': False,
            'kvm_openstack': True,
            'ipsec': False,
            'mtu': 1400,
            'ovn_hostrouting': False,
            'manifests': 'manifests',
            'sno': False,
            'sno_ctlplanes': False,
            'sno_workers': False,
            'sno_wait': False,
            'sno_localhost_fix': False,
            'sno_disable_nics': [],
            'sno_cpuset': None,
            'sno_relocate': False,
            'notify': False,
            'async': False,
            'kubevirt_api_service': False,
            'kubevirt_ignore_node_port': False,
            'baremetal_web': True,
            'baremetal_web_dir': '/var/www/html',
            'baremetal_web_port': 80,
            'baremetal_cidr': None,
            'coredns': True,
            'mdns': True,
            'sslip': False,
            'autoscale': False,
            'upstream': False,
            'calico_version': None,
            'contrail_version': '23.1',
            'contrail_ctl_network': 'contrail-ctl',
            'contrail_ctl_create': True,
            'contrail_ctl_cidr': '10.40.1.0/24',
            'contrail_ctl_gateway': '10.40.1.1',
            'disconnected_deploy': False,
            'disconnected_update': False,
            'disconnected_reuse': False,
            'disconnected_operators_all': False,
            'disconnected_operators': [],
            'disconnected_operators_version': None,
            'disconnected_certified_operators_all': False,
            'disconnected_certified_operators': [],
            'disconnected_certified_operators_version': None,
            'disconnected_community_operators_all': False,
            'disconnected_community_operators': [],
            'disconnected_community_operators_version': None,
            'disconnected_marketplace_operators_all': False,
            'disconnected_marketplace_operators': [],
            'disconnected_marketplace_operators_version': None,
            'disconnected_extra_catalogs': [],
            'retries': 2}
    data.update(overrides)
    clustervalue = overrides.get('cluster') or cluster or 'myopenshift'
    if data['ctlplanes'] == 1 and data['workers'] == 0\
       and 'ctlplane_memory' not in overrides and 'memory' not in overrides:
        overrides['ctlplane_memory'] = 32768
        warning("Forcing memory of single ctlplane vm to 32G")
    retries = data.get('retries')
    data['cluster'] = clustervalue
    domain = data.get('domain')
    original_domain = None
    async_install = data.get('async')
    upstream = data.get('upstream')
    autoscale = data.get('autoscale')
    sslip = data.get('sslip')
    baremetal_hosts = data.get('baremetal_hosts', [])
    notify = data.get('notify')
    postscripts = data.get('postscripts', [])
    pprint(f"Deploying cluster {clustervalue}")
    plan = cluster if cluster is not None else clustervalue
    overrides['kubetype'] = 'openshift'
    apps = overrides.get('apps', [])
    if ('localstorage' in apps or 'ocs' in apps) and 'extra_disks' not in overrides\
            and 'extra_ctlplane_disks' not in overrides and 'extra_worker_disks' not in overrides:
        warning("Storage apps require extra disks to be set")
    overrides['kube'] = data['cluster']
    installparam = overrides.copy()
    installparam['cluster'] = clustervalue
    sno = data.get('sno', False)
    ignore_hosts = data.get('ignore_hosts', False)
    if sno:
        sno_ctlplanes = data.get('sno_ctlplanes')
        sno_workers = data.get('sno_workers')
        sno_wait = data.get('sno_wait')
        sno_disk = data.get('sno_disk')
        if sno_disk is None:
            warning("sno_disk will be discovered")
        ctlplanes = 1
        workers = 0
        data['mdns'] = False
        data['kubetype'] = 'openshift'
        data['kube'] = data['cluster']
        if data.get('network_type', 'OVNKubernetes') == 'OpenShiftSDN':
            warning("Forcing network_type to OVNKubernetes")
            data['network_type'] = 'OVNKubernetes'
    ctlplanes = data.get('ctlplanes', 1)
    if ctlplanes == 0:
        msg = "Invalid number of ctlplanes"
        return {'result': 'failure', 'reason': msg}
    network = data.get('network')
    ipv6 = data['ipv6']
    disconnected_deploy = data['disconnected_deploy']
    disconnected_update = data['disconnected_update']
    disconnected_reuse = data['disconnected_reuse']
    disconnected_operators = data.get('disconnected_operators', [])
    disconnected_certified_operators = data['disconnected_certified_operators']
    disconnected_community_operators = data['disconnected_community_operators']
    disconnected_marketplace_operators = data['disconnected_marketplace_operators']
    disconnected_url = data.get('disconnected_url')
    disconnected_user = data.get('disconnected_user')
    disconnected_password = data.get('disconnected_password')
    ipsec = data.get('ipsec')
    mtu = data.get('mtu')
    ovn_hostrouting = data.get('ovn_hostrouting')
    metal3 = data.get('metal3')
    if not data.get('coredns'):
        warning("You will need to provide DNS records for api and ingress on your own")
    mdns = data.get('mdns')
    sno_localhost_fix = data.get('sno_localhost_fix')
    sno_cpuset = data.get('sno_cpuset')
    sno_relocate = data.get('sno_relocate')
    kubevirt_api_service, kubevirt_api_service_node_port = False, False
    kubevirt_ignore_node_port = data['kubevirt_ignore_node_port']
    version = data.get('version')
    tag = data.get('tag')
    if str(tag) == '4.1':
        tag = '4.10'
        data['tag'] = tag
    if os.path.exists('coreos-installer'):
        pprint("Removing old coreos-installer")
        os.remove('coreos-installer')
    if version not in ['ci', 'dev-preview', 'nightly', 'stable']:
        msg = f"Incorrect version {version}"
        return {'result': 'failure', 'reason': msg}
    else:
        pprint(f"Using {version} version")
    cluster = data.get('cluster')
    image = data.get('image')
    api_ip = data.get('api_ip')
    cidr = None
    if platform in virtplatforms and not sno and api_ip is None:
        network = data.get('network')
        networkinfo = k.info_network(network)
        if not networkinfo:
            msg = f"Issue getting network {network}"
            return {'result': 'failure', 'reason': msg}
        if platform == 'kvm' and networkinfo['type'] == 'routed':
            cidr = networkinfo['cidr']
            if cidr == 'N/A':
                msg = "Couldnt gather an api_ip from your specified network"
                return {'result': 'failure', 'reason': msg}
            api_index = 2 if ':' in cidr else -3
            api_ip = str(ip_network(cidr)[api_index])
            warning(f"Using {api_ip} as api_ip")
            overrides['api_ip'] = api_ip
        elif platform == 'kubevirt':
            selector = {'kcli/plan': plan, 'kcli/role': 'ctlplane'}
            service_type = "LoadBalancer" if k.access_mode == 'LoadBalancer' else 'NodePort'
            if service_type == 'NodePort':
                kubevirt_api_service_node_port = True
            api_ip = k.create_service(f"{cluster}-api", k.namespace, selector, _type=service_type,
                                      ports=[6443, 22623, 22624, 80, 443], openshift_hack=True)
            if api_ip is None:
                msg = "Couldnt gather an api_ip from your specified network"
                return {'result': 'failure', 'reason': msg}
            else:
                pprint(f"Using api_ip {api_ip}")
                overrides['api_ip'] = api_ip
                overrides['kubevirt_api_service'] = True
                kubevirt_api_service = True
                overrides['mdns'] = False
        else:
            msg = "You need to define api_ip in your parameters file"
            return {'result': 'failure', 'reason': msg}
    if platform in virtplatforms and not sno and ':' in api_ip:
        ipv6 = True
    if ipv6:
        if data.get('network_type', 'OVNKubernetes') == 'OpenShiftSDN':
            warning("Forcing network_type to OVNKubernetes")
            data['network_type'] = 'OVNKubernetes'
        data['ipv6'] = True
        overrides['ipv6'] = True
        data['disconnected_ipv6_network'] = True
        if not disconnected_deploy and disconnected_url is None:
            warning("Forcing disconnected_deploy to True as no disconnected_url was provided")
            data['disconnected_deploy'] = True
            disconnected_deploy = True
        if sno and not data['dualstack'] and 'extra_args' not in overrides:
            warning("Forcing extra_args to ip=dhcp6 for sno to boot with ipv6")
            data['extra_args'] = 'ip=dhcp6'
    ingress_ip = data.get('ingress_ip')
    if ingress_ip is not None and api_ip is not None and ingress_ip == api_ip:
        ingress_ip = None
        overrides['ingress_ip'] = None
    if sslip and platform in virtplatforms:
        original_domain = domain
        domain = '%s.sslip.io' % api_ip.replace('.', '-').replace(':', '-')
        data['domain'] = domain
        pprint(f"Setting domain to {domain}")
        ignore_hosts = False
    public_api_ip = data.get('public_api_ip')
    provider_network = False
    network = data.get('network')
    ctlplanes = data.get('ctlplanes')
    workers = data.get('workers')
    tag = data.get('tag')
    pull_secret = pwd_path(data.get('pull_secret')) if not upstream else f"{plandir}/fake_pull.json"
    pull_secret = os.path.expanduser(pull_secret)
    macosx = data.get('macosx')
    if macosx and not os.path.exists('/i_am_a_container'):
        macosx = False
    if platform == 'openstack' and not sno:
        if data.get('flavor') is None:
            msg = "Missing flavor in parameter file"
            return {'result': 'failure', 'reason': msg}
        provider_network = k.provider_network(network)
        if not provider_network:
            if api_ip is None:
                cidr = k.info_network(network)['cidr']
                api_ip = str(ip_network(cidr)[-3])
                data['api_ip'] = api_ip
                warning(f"Using {api_ip} as api_ip")
            if public_api_ip is None:
                public_api_ip = config.k.create_network_port(f"{cluster}-vip", network, ip=api_ip,
                                                             floating=True)['floating']
    if not os.path.exists(pull_secret):
        msg = f"Missing pull secret file {pull_secret}"
        return {'result': 'failure', 'reason': msg}
    if which('oc') is None:
        get_oc(macosx=macosx)
    pub_key = data.get('pub_key') or get_ssh_pub_key()
    keys = data.get('keys', [])
    if pub_key is None:
        if keys:
            warning("Using first key from your keys array")
            pub_key = keys[0]
        else:
            msg = "No usable public key found, which is required for the deployment. Create one using ssh-keygen"
            return {'result': 'failure', 'reason': msg}
    pub_key = os.path.expanduser(pub_key)
    if pub_key.startswith('ssh-'):
        data['pub_key'] = pub_key
    elif os.path.exists(pub_key):
        data['pub_key'] = open(pub_key).read().strip()
    else:
        msg = f"Publickey file {pub_key} not found"
        return {'result': 'failure', 'reason': msg}
    clusterdir = os.path.expanduser(f"~/.kcli/clusters/{cluster}")
    if os.path.exists(clusterdir):
        if [v for v in config.k.list() if v.get('plan', 'kvirt') == cluster]:
            msg = f"Remove existing directory {clusterdir} or use --force"
            return {'result': 'failure', 'reason': msg}
        else:
            pprint(f"Removing existing directory {clusterdir}")
            rmtree(clusterdir)
    os.environ['KUBECONFIG'] = f"{clusterdir}/auth/kubeconfig"
    if version == 'ci':
        if '/' not in str(tag):
            if arch in ['aarch64', 'arm64']:
                tag = f'registry.ci.openshift.org/ocp-arm64/release-arm64:{tag}'
            else:
                basetag = 'ocp'
                tag = f'registry.ci.openshift.org/{basetag}/release:{tag}'
        os.environ['OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE'] = tag
        pprint(f"Setting OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE to {tag}")
    which_openshift = which('openshift-install')
    openshift_dir = os.path.dirname(which_openshift) if which_openshift is not None else '.'
    if upstream:
        run = get_upstream_installer(tag, version=version)
    elif not same_release_images(version=version, tag=tag, pull_secret=pull_secret, path=openshift_dir):
        if version in ['ci', 'nightly'] or '/' in str(tag):
            nightly = version == 'nigthly'
            run = get_ci_installer(pull_secret, tag=tag, nightly=nightly)
        elif version == 'dev-preview':
            run = get_downstream_installer(devpreview=True, tag=tag, pull_secret=pull_secret)
        else:
            run = get_downstream_installer(tag=tag, pull_secret=pull_secret)
        if run != 0:
            msg = "Couldn't download openshift-install"
            return {'result': 'failure', 'reason': msg}
        pprint("Move downloaded openshift-install somewhere in your PATH if you want to reuse it")
    elif which_openshift is not None:
        pprint("Using existing openshift-install found in your PATH")
    else:
        pprint("Reusing matching openshift-install")
    os.environ["PATH"] = f'{os.getcwd()}:{os.environ["PATH"]}'
    if disconnected_url is not None:
        if disconnected_user is None:
            msg = "disconnected_user needs to be set"
            return {'result': 'failure', 'reason': msg}
        if disconnected_password is None:
            msg = "disconnected_password needs to be set"
            return {'result': 'failure', 'reason': msg}
        if disconnected_url.startswith('http'):
            warning(f"Removing scheme from {disconnected_url}")
            disconnected_url = disconnected_url.replace('http://', '').replace('https://', '')
        update_pull_secret(pull_secret, disconnected_url, disconnected_user, disconnected_password)
        ori_tag = tag
        if '/' not in str(tag):
            tag = f'{disconnected_url}/openshift/release-images:{tag}'
            os.environ['OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE'] = tag
        pprint(f"Setting OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE to {tag}")
        data['openshift_release_image'] = tag
        if 'ca' not in data and 'quay.io' not in disconnected_url:
            pprint(f"Trying to gather registry ca cert from {disconnected_url}")
            cacmd = f"openssl s_client -showcerts -connect {disconnected_url} </dev/null 2>/dev/null|"
            cacmd += "openssl x509 -outform PEM"
            data['ca'] = os.popen(cacmd).read()
    INSTALLER_VERSION = get_installer_version()
    COMMIT_ID = os.popen('openshift-install version').readlines()[1].replace('built from commit', '').strip()
    pprint(f"Using installer version {INSTALLER_VERSION}")
    if sno:
        pass
    elif image is None:
        image_type = config.type
        if data.get('kvm_openstack') and config.type == 'kvm':
            image_type = 'openstack'
        if config.type == "proxmox":
            image_type = 'kvm'
        region = config.k.region if config.type == 'aws' else None
        try:
            if upstream:
                fcos_url = 'https://builds.coreos.fedoraproject.org/streams/stable.json'
                image_url = get_latest_fcos(fcos_url, _type=image_type)
            else:
                try:
                    image_url = get_installer_rhcos(_type=image_type, region=region, arch=arch)
                except:
                    image_url = get_commit_rhcos(COMMIT_ID, _type=image_type, region=region)
        except:
            msg = f"Couldn't gather the {config.type} image associated to commit {COMMIT_ID}. "
            msg += "Force an image in your parameter file"
            return {'result': 'failure', 'reason': msg}
        if platform in ['aws', 'gcp']:
            image = image_url
        else:
            image = os.path.basename(os.path.splitext(image_url)[0])
            if platform in ['ibm', 'kubevirt', 'proxmox']:
                image = image.replace('.', '-').replace('_', '-').lower()
            if platform == 'vsphere':
                image = image.replace(f'.{arch}', '')
            images = [v for v in k.volumes() if image in v]
            if not images:
                result = config.handle_host(pool=config.pool, image=image, download=True,
                                            url=image_url, size=data.get('kubevirt_disk_size'))
                if result['result'] != 'success':
                    return result
        pprint(f"Using image {image}")
    elif config.type == 'kubevirt' and '/' in image:
        warning(f"Assuming image {image} is available")
    else:
        pprint(f"Checking if image {image} is available")
        images = [v for v in k.volumes() if image in v]
        if not images:
            msg = f"Missing {image}. Indicate correct image in your parameters file..."
            return {'result': 'failure', 'reason': msg}
    overrides['image'] = image
    static_networking_ctlplane, static_networking_worker = False, False
    macentries = []
    vmrules = overrides.get('vmrules', [])
    for entry in vmrules:
        if isinstance(entry, dict):
            hostname = list(entry.keys())[0]
            if isinstance(entry[hostname], dict):
                rule = entry[hostname]
                if 'nets' in rule and isinstance(rule['nets'], list):
                    netrule = rule['nets'][0]
                    if isinstance(netrule, dict) and 'ip' in netrule and 'netmask' in netrule:
                        mac, ip = netrule.get('mac'), netrule['ip']
                        netmask, gateway = netrule['netmask'], netrule.get('gateway')
                        nameserver = netrule.get('dns', gateway)
                        if mac is not None and gateway is not None:
                            macentries.append(f"{mac};{hostname};{ip};{netmask};{gateway};{nameserver}")
                        if hostname.startswith(f"{cluster}-ctlplane"):
                            static_networking_ctlplane = True
                        elif hostname.startswith(f"{cluster}-worker"):
                            static_networking_worker = True
    overrides['cluster'] = cluster
    if not os.path.exists(clusterdir):
        os.makedirs(clusterdir)
    if platform in virtplatforms and disconnected_deploy:
        disconnected_vm = f"{data.get('disconnected_reuse_name', cluster)}-disconnected"
        pprint(f"Deploying disconnected vm {disconnected_vm}")
        data['pull_secret'] = re.sub(r"\s", "", open(pull_secret).read())
        disconnected_plan = f"{plan}-reuse" if disconnected_reuse else plan
        disconnected_overrides = data.copy()
        disconnected_overrides['kube'] = f"{cluster}-reuse" if disconnected_reuse else cluster
        disconnected_overrides['openshift_version'] = INSTALLER_VERSION
        disconnected_overrides['disconnected_operators_version'] = '.'.join(INSTALLER_VERSION.split('.')[:-1])
        disconnected_overrides['openshift_release_image'] = get_release_image()
        data['openshift_release_image'] = disconnected_overrides['openshift_release_image']
        x_apps = ['users', 'autolabeller', 'metal3', 'nfs']
        for app in apps:
            if app not in x_apps and app not in disconnected_operators:
                warning(f"Adding app {app} to disconnected_operators array")
                disconnected_operators.append(app)
        disconnected_overrides['disconnected_operators'] = disconnected_operators
        result = config.plan(disconnected_plan, inputfile=f'{plandir}/disconnected.yml',
                             overrides=disconnected_overrides)
        if result['result'] != 'success':
            return result
        disconnected_ip, disconnected_vmport = _ssh_credentials(k, disconnected_vm)[1:]
        cacmd = "cat /opt/registry/certs/domain.crt"
        cacmd = ssh(disconnected_vm, ip=disconnected_ip, user='root', tunnel=config.tunnel,
                    tunnelhost=config.tunnelhost, tunnelport=config.tunnelport, tunneluser=config.tunneluser,
                    insecure=True, cmd=cacmd, vmport=disconnected_vmport)
        disconnected_ca = os.popen(cacmd).read().strip()
        if data.get('ca') is not None:
            data['ca'] += disconnected_ca
        else:
            data['ca'] = disconnected_ca
        urlcmd = "cat /root/url.txt"
        urlcmd = ssh(disconnected_vm, ip=disconnected_ip, user='root', tunnel=config.tunnel,
                     tunnelhost=config.tunnelhost, tunnelport=config.tunnelport, tunneluser=config.tunneluser,
                     insecure=True, cmd=urlcmd, vmport=disconnected_vmport)
        disconnected_url = os.popen(urlcmd).read().strip()
        overrides['disconnected_url'] = disconnected_url
        data['disconnected_url'] = disconnected_url
        if disconnected_user is None:
            disconnected_user = 'dummy'
        if disconnected_password is None:
            disconnected_password = 'dummy'
        versioncmd = "cat /root/version.txt"
        versioncmd = ssh(disconnected_vm, ip=disconnected_ip, user='root', tunnel=config.tunnel,
                         tunnelhost=config.tunnelhost, tunnelport=config.tunnelport, tunneluser=config.tunneluser,
                         insecure=True, cmd=versioncmd, vmport=disconnected_vmport)
        disconnected_version = os.popen(versioncmd).read().strip()
        if disconnected_operators or disconnected_certified_operators or disconnected_community_operators or\
           disconnected_marketplace_operators:
            source = "/root/imageContentSourcePolicy.yaml"
            destination = f"{clusterdir}/imageContentSourcePolicy.yaml"
            scpcmd = scp(disconnected_vm, ip=disconnected_ip, user='root', source=source,
                         destination=destination, tunnel=config.tunnel, tunnelhost=config.tunnelhost,
                         tunnelport=config.tunnelport, tunneluser=config.tunneluser, download=True, insecure=True,
                         vmport=disconnected_vmport)
            os.system(scpcmd)
        if disconnected_operators:
            source = "/root/catalogSource-redhat-operator-index.yaml"
            destination = f"{clusterdir}/catalogSource-redhat.yaml"
            scpcmd = scp(disconnected_vm, ip=disconnected_ip, user='root', source=source,
                         destination=destination, tunnel=config.tunnel, tunnelhost=config.tunnelhost,
                         tunnelport=config.tunnelport, tunneluser=config.tunneluser, download=True, insecure=True,
                         vmport=disconnected_vmport)
            os.system(scpcmd)
        if disconnected_certified_operators:
            source = "/root/catalogSource-certified-operator-index.yaml"
            destination = f"{clusterdir}/catalogSource-certified.yaml"
            scpcmd = scp(disconnected_vm, ip=disconnected_ip, user='root', source=source,
                         destination=destination, tunnel=config.tunnel, tunnelhost=config.tunnelhost,
                         tunnelport=config.tunnelport, tunneluser=config.tunneluser, download=True, insecure=True,
                         vmport=disconnected_vmport)
            os.system(scpcmd)
        if disconnected_community_operators:
            source = "/root/catalogSource-community-operator-index.yaml"
            destination = f"{clusterdir}/catalogSource-community.yaml"
            scpcmd = scp(disconnected_vm, ip=disconnected_ip, user='root', source=source,
                         destination=destination, tunnel=config.tunnel, tunnelhost=config.tunnelhost,
                         tunnelport=config.tunnelport, tunneluser=config.tunneluser, download=True, insecure=True,
                         vmport=disconnected_vmport)
            os.system(scpcmd)
        if disconnected_marketplace_operators:
            source = "/root/catalogSource-redhat-marketplace-index.yaml"
            destination = f"{clusterdir}/catalogSource-marketplace.yaml"
            scpcmd = scp(disconnected_vm, ip=disconnected_ip, user='root', source=source,
                         destination=destination, tunnel=config.tunnel, tunnelhost=config.tunnelhost,
                         tunnelport=config.tunnelport, tunneluser=config.tunneluser, download=True, insecure=True,
                         vmport=disconnected_vmport)
            os.system(scpcmd)
        os.environ['OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE'] = disconnected_version
        pprint(f"Setting OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE to {disconnected_version}")
    if disconnected_url is not None:
        if disconnected_update:
            with open(f'{clusterdir}/ca.crt', 'w') as f:
                f.write(data['ca'])
            call(f'sudo cp {clusterdir}/ca.crt /etc/pki/ca-trust/source/anchors ; sudo update-ca-trust extract',
                 shell=True)
            pprint("Updating disconnected registry")
            synccmd = f"oc adm release mirror -a {pull_secret} --from={get_release_image()} "
            synccmd += f"--to-release-image={disconnected_url}/openshift/release-images:{ori_tag} "
            synccmd += f"--to={disconnected_url}/openshift/release"
            pprint(f"Running {synccmd}")
            call(synccmd, shell=True)
            if which('oc-mirror') is None:
                get_oc_mirror(version=overrides.get('version', 'stable'), tag=overrides.get('tag', '4.12'))
            else:
                warning("Using oc-mirror from your PATH")
            mirror_data = data.copy()
            mirror_data['tag'] = ori_tag
            extra_images = data.get('disconnected_extra_images', [])
            kcli_images = ['curl', 'haproxy', 'kubectl', 'mdns-publisher', 'origin-coredns',
                           'origin-keepalived-ipfailover']
            kcli_images = [f'quay.io/karmab/{image}:latest' for image in kcli_images]
            extra_images.extend(kcli_images)
            mirror_data['extra_images'] = [*set(extra_images)]
            mirrorconf = config.process_inputfile(cluster, f"{plandir}/disconnected/scripts/mirror-config.yaml.sample",
                                                  overrides=mirror_data)
            with open(f"{clusterdir}/mirror-config.yaml", 'w') as f:
                f.write(mirrorconf)
            dockerdir = os.path.expanduser('~/.docker')
            if not os.path.isdir(dockerdir):
                os.mkdir(dockerdir)
            copy2(pull_secret, f"{dockerdir}/config.json")
            olmcmd = f"oc-mirror --ignore-history --config {clusterdir}/mirror-config.yaml docker://{disconnected_url}"
            olmcmd = ' || '.join([olmcmd for x in range(3)])
            pprint(f"Running {olmcmd}")
            call(olmcmd, shell=True)
            mapping_to_icsp(config, plandir, f"{clusterdir}", f"{clusterdir}/mirror-config.yaml")
            for catalogsource in glob("oc-mirror-workspace/results-*/catalogSource*.yaml"):
                pprint(f"Injecting catalogsource {catalogsource}")
                copy2(catalogsource, clusterdir)
            for icsp in glob("oc-mirror-workspace/results-*/imageContentSourcePolicy.yaml"):
                pprint(f"Injecting icsp {icsp}")
                copy2(icsp, clusterdir)
            if os.path.exists(f"{clusterdir}/imageContentSourcePolicy.yaml"):
                separate_yamls(f"{clusterdir}/imageContentSourcePolicy.yaml")
            if os.path.exists("oc-mirror-workspace"):
                rmtree("oc-mirror-workspace")
        key = f"{disconnected_user}:{disconnected_password}"
        key = str(b64encode(key.encode('utf-8')), 'utf-8')
        auths = {'auths': {disconnected_url: {'auth': key, 'email': 'jhendrix@karmalabs.corp'}}}
        data['pull_secret'] = json.dumps(auths)
    else:
        data['pull_secret'] = re.sub(r"\s", "", open(pull_secret).read())
    if config.type == 'aws':
        aws_credentials(config)
    elif config.type == 'gcp':
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.expanduser(config.options.get('credentials'))
    installconfig = config.process_inputfile(cluster, f"{plandir}/install-config.yaml", overrides=data)
    with open(f"{clusterdir}/install-config.yaml", 'w') as f:
        f.write(installconfig)
    with open(f"{clusterdir}/install-config.yaml.bck", 'w') as f:
        f.write(installconfig)
    run = call(f'openshift-install --dir={clusterdir} --log-level={log_level} create manifests', shell=True)
    if run != 0:
        msg = "Leaving environment for debugging purposes. "
        msg += f"Delete it with kcli delete kube --yes {cluster}"
        return {'result': 'failure', 'reason': msg}
    for f in glob(f"{clusterdir}/openshift/99_openshift-cluster-api_master-machines-*.yaml"):
        os.remove(f)
    for f in glob(f"{clusterdir}/openshift/99_openshift-cluster-api_worker-machineset-*"):
        os.remove(f)
    for f in glob(f"{clusterdir}/openshift/99_openshift-machine-api_master-control-plane-machine-set.yaml"):
        os.remove(f)
    ntp_server = data.get('ntp_server')
    if ntp_server is not None:
        ntp_data = config.process_inputfile(cluster, f"{plandir}/chrony.conf", overrides={'ntp_server': ntp_server})
        for role in ['master', 'worker']:
            ntp = config.process_inputfile(cluster, f"{plandir}/99-chrony.yaml",
                                           overrides={'role': role, 'ntp_data': ntp_data})
            with open(f"{clusterdir}/manifests/99-chrony-{role}.yaml", 'w') as f:
                f.write(ntp)
    baremetal_cidr = data.get('baremetal_cidr')
    if baremetal_cidr is not None:
        node_ip_hint = f"KUBELET_NODEIP_HINT={baremetal_cidr.split('/')[0]}"
        for role in ['master', 'worker']:
            hint = config.process_inputfile(cluster, f"{plandir}/10-node-ip-hint.yaml",
                                            overrides={'role': role, 'node_ip_hint': node_ip_hint})
            with open(f"{clusterdir}/manifests/99-chrony-{role}.yaml", 'w') as f:
                f.write(hint)
    manifestsdir = data.get('manifests')
    manifestsdir = pwd_path(manifestsdir)
    if os.path.exists(manifestsdir) and os.path.isdir(manifestsdir):
        for f in glob(f"{manifestsdir}/*.y*ml"):
            pprint(f"Injecting manifest {f}")
            copy2(f, f"{clusterdir}/openshift")
    elif isinstance(manifestsdir, list):
        for manifest in manifestsdir:
            f, content = list(manifest.keys())[0], list(manifest.values())[0]
            if not f.endswith('.yml') and not f.endswith('.yaml'):
                warning(f"Skipping manifest {f}")
                continue
            pprint(f"Injecting manifest {f}")
            with open(f'{clusterdir}/openshift/{f}', 'w') as f:
                f.write(content)
    for yamlfile in glob(f"{clusterdir}/*.yaml"):
        if os.stat(yamlfile).st_size == 0:
            warning(f"Skipping empty file {yamlfile}")
        elif 'catalogSource' in yamlfile or 'imageContentSourcePolicy' in yamlfile:
            copy2(yamlfile, f"{clusterdir}/openshift")
    if 'network_type' in data:
        if data['network_type'] == 'Calico':
            calico_version = data['calico_version']
            with TemporaryDirectory() as tmpdir:
                calico_data = {'tmpdir': tmpdir, 'clusterdir': clusterdir, 'calico_version': calico_version}
                calico_script = config.process_inputfile('xxx', f'{plandir}/calico.sh.j2', overrides=calico_data)
                with open(f"{tmpdir}/calico.sh", 'w') as f:
                    f.write(calico_script)
                call(f'bash {tmpdir}/calico.sh', shell=True)
        if data['network_type'] == 'Contrail':
            if which('git') is None:
                return {'result': 'failure', 'reason': "Git is needed when deploying with contrail"}
            if 'enterprise-hub.juniper.net' not in data['pull_secret']:
                return {'result': 'failure', 'reason': "A token for hub.juniper.net registry is needed"}
            ctl_network_create = data['contrail_ctl_create']
            ctl_network = data['contrail_ctl_network']
            ctl_cidr = data['contrail_ctl_cidr']
            networkinfo = k.info_network(ctl_network)
            if not networkinfo:
                if ctl_network_create:
                    result = k.create_network(ctl_network, cidr=ctl_cidr, plan=plan)
                    if result['result'] != 'success':
                        return result
                else:
                    msg = f"Issue getting contrail ctl network {ctl_network}"
                    return {'result': 'failure', 'reason': msg}
            elif platform == 'kvm' and networkinfo['type'] == 'routed':
                cidr = networkinfo['cidr']
                if cidr == 'N/A':
                    msg = "Couldnt gather cidr from your specified contrail ctl network"
                    return {'result': 'failure', 'reason': msg}
                elif cidr != ctl_cidr:
                    msg = "Contrail ctl network cidr doesnt match contrail_ctl_cidr"
                    return {'result': 'failure', 'reason': msg}
            if 'uefi' in data and data['uefi']:
                data['secureboot'] = True
            with TemporaryDirectory() as tmpdir:
                contrail_data = {'tmpdir': tmpdir, 'clusterdir': clusterdir, 'uefi': data.get('uefi', False)}
                contrail_data.update({key: data[key] for key in data if key.startswith('contrail')})
                contrail_data['auth'] = yaml.safe_load(open(pull_secret))['auths']['enterprise-hub.juniper.net']['auth']
                contrail_script = config.process_inputfile('xxx', f'{plandir}/contrail.sh.j2', overrides=contrail_data)
                with open(f"{tmpdir}/contrail.sh", 'w') as f:
                    f.write(contrail_script)
                copy2(f'{plandir}/contrail.auth', tmpdir)
                call(f'bash {tmpdir}/contrail.sh', shell=True)
    if ipsec or ovn_hostrouting or sno_relocate or mtu != 1400:
        ovn_data = config.process_inputfile(cluster, f"{plandir}/99-ovn.yaml",
                                            overrides={'ipsec': ipsec, 'ovn_hostrouting': ovn_hostrouting,
                                                       'relocate': sno_relocate, 'mtu': mtu})
        with open(f"{clusterdir}/openshift/99-ovn.yaml", 'w') as f:
            f.write(ovn_data)
    if workers == 0 or not mdns or kubevirt_api_service:
        copy2(f'{plandir}/cluster-scheduler-02-config.yml', f"{clusterdir}/manifests")
    if disconnected_operators:
        if os.path.exists(f'{clusterdir}/imageContentSourcePolicy.yaml'):
            copy2(f'{clusterdir}/imageContentSourcePolicy.yaml', f"{clusterdir}/openshift")
        if os.path.exists(f'{clusterdir}/catalogsource.yaml'):
            copy2(f'{clusterdir}/catalogsource.yaml', f"{clusterdir}/openshift")
        copy2(f'{plandir}/99-operatorhub.yaml', f"{clusterdir}/openshift")
    if 'sslip' in domain:
        ingress_sslip_data = config.process_inputfile(cluster, f"{plandir}/cluster-ingress-02-config.yml",
                                                      overrides={'cluster': cluster, 'domain': domain})
        with open(f"{clusterdir}/manifests/cluster-ingress-02-config.yml", 'w') as f:
            f.write(ingress_sslip_data)
    cron_overrides = {'registry': disconnected_url or 'quay.io'}
    cron_overrides['version'] = 'v1beta1' if get_installer_minor(INSTALLER_VERSION) < 8 else 'v1'
    autoapproverdata = config.process_inputfile(cluster, f"{plandir}/autoapprovercron.yml", overrides=cron_overrides)
    with open(f"{clusterdir}/autoapprovercron.yml", 'w') as f:
        f.write(autoapproverdata)
    for f in glob(f"{plandir}/customisation/*.yaml"):
        if '99-ingress-controller.yaml' in f:
            ingressrole = 'master' if workers == 0 or not mdns or kubevirt_api_service else 'worker'
            replicas = 1 if sno else ctlplanes if workers == 0 or not mdns or kubevirt_api_service else workers
            if platform in virtplatforms and sslip and ingress_ip is None:
                replicas = ctlplanes
                ingressrole = 'master'
                warning("Forcing router pods on ctlplanes since sslip is set and api_ip will be used for ingress")
                copy2(f'{plandir}/cluster-scheduler-02-config.yml', f"{clusterdir}/manifests")
            ingressconfig = config.process_inputfile(cluster, f, overrides={'replicas': replicas, 'role': ingressrole,
                                                                            'cluster': cluster, 'domain': domain})
            with open(f"{clusterdir}/openshift/99-ingress-controller.yaml", 'w') as _f:
                _f.write(ingressconfig)
            continue
        if '99-autoapprovercron-cronjob.yaml' in f:
            crondata = config.process_inputfile(cluster, f, overrides=cron_overrides)
            with open(f"{clusterdir}/openshift/99-autoapprovercron-cronjob.yaml", 'w') as _f:
                _f.write(crondata)
            continue
        if '99-monitoring.yaml' in f:
            monitoring_retention = data['monitoring_retention']
            monitoringfile = config.process_inputfile(cluster, f, overrides={'retention': monitoring_retention})
            with open(f"{clusterdir}/openshift/99-monitoring.yaml", 'w') as _f:
                _f.write(monitoringfile)
            continue
        copy2(f, f"{clusterdir}/openshift")
    registry = disconnected_url or 'quay.io'
    if async_install or autoscale:
        config.import_in_kube(network=network, dest=f"{clusterdir}/openshift", secure=True)
        deletionfile = f"{plandir}/99-bootstrap-deletion.yaml"
        deletionfile = config.process_inputfile(cluster, deletionfile, overrides={'cluster': cluster,
                                                                                  'registry': registry,
                                                                                  'client': config.client})
        with open(f"{clusterdir}/openshift/99-bootstrap-deletion.yaml", 'w') as _f:
            _f.write(deletionfile)
        if not autoscale:
            deletionfile2 = f"{plandir}/99-bootstrap-deletion-2.yaml"
            deletionfile2 = config.process_inputfile(cluster, deletionfile2, overrides={'registry': registry})
            with open(f"{clusterdir}/openshift/99-bootstrap-deletion-2.yaml", 'w') as _f:
                _f.write(deletionfile2)
    if notify and (async_install or (sno and not sno_wait)):
        notifycmd = "cat /shared/results.txt"
        notifycmds, mailcontent = config.handle_notifications(cluster, notifymethods=config.notifymethods,
                                                              pushbullettoken=config.pushbullettoken,
                                                              notifycmd=notifycmd, slackchannel=config.slackchannel,
                                                              slacktoken=config.slacktoken,
                                                              mailserver=config.mailserver,
                                                              mailfrom=config.mailfrom, mailto=config.mailto,
                                                              cluster=True)
        notifyfile = f"{plandir}/99-notifications.yaml"
        notifyfile = config.process_inputfile(cluster, notifyfile, overrides={'registry': registry,
                                                                              'cluster': cluster,
                                                                              'domain': original_domain,
                                                                              'sno': sno,
                                                                              'cmds': notifycmds,
                                                                              'mailcontent': mailcontent})
        with open(f"{clusterdir}/openshift/99-notifications.yaml", 'w') as _f:
            _f.write(notifyfile)
    if apps and (async_install or (sno and not sno_wait)):
        registry = disconnected_url or 'quay.io'
        appsfile = f"{plandir}/99-apps.yaml"
        apps_data = {'registry': registry, 'overrides': overrides, 'overrides_string': yaml.dump(overrides)}
        appsfile = config.process_inputfile(cluster, appsfile, overrides=apps_data)
        with open(f"{clusterdir}/openshift/99-apps.yaml", 'w') as _f:
            _f.write(appsfile)
    if metal3:
        copy2(f"{plandir}/99-metal3-provisioning.yaml", f"{clusterdir}/openshift")
        copy2(f"{plandir}/99-metal3-fake-machine.yaml", f"{clusterdir}/openshift")
    if config.type == 'kubevirt':
        kubevirtctlplane = config.process_inputfile(cluster, f"{plandir}/99-kubevirt-fix.yaml",
                                                    overrides={'role': 'master'})
        with open(f"{clusterdir}/openshift/99-kubevirt-fix-ctlplane.yaml", 'w') as _f:
            _f.write(kubevirtctlplane)
        kubevirtworker = config.process_inputfile(cluster, f"{plandir}/99-kubevirt-fix.yaml",
                                                  overrides={'role': 'worker'})
        with open(f"{clusterdir}/openshift/99-kubevirt-fix-worker.yaml", 'w') as _f:
            _f.write(kubevirtworker)
    if sno:
        sno_name = f"{cluster}-sno"
        sno_files = []
        sno_disable_nics = data.get('sno_disable_nics', [])
        if ipv6 or sno_disable_nics:
            nm_data = config.process_inputfile(cluster, f"{plandir}/kcli-ipv6.conf.j2", overrides=data)
            sno_files.append({'path': "/etc/NetworkManager/conf.d/kcli-ipv6.conf", 'data': nm_data})
        sno_dns = data.get('sno_dns')
        if sno_dns is None:
            sno_dns = False
            for entry in [f'api-int.{cluster}.{domain}', f'api.{cluster}.{domain}', f'xxx.apps.{cluster}.{domain}']:
                try:
                    gethostbyname(entry)
                except:
                    sno_dns = True
            data['sno_dns'] = sno_dns
        if sno_dns:
            warning("Injecting coredns static pod as some DNS records were missing")
            coredns_data = config.process_inputfile(cluster, f"{plandir}/staticpods/coredns.yml", overrides=data)
            corefile_data = config.process_inputfile(cluster, f"{plandir}/Corefile", overrides=data)
            forcedns_data = config.process_inputfile(cluster, f"{plandir}/99-forcedns", overrides=data)
            sno_files.extend([{'path': "/etc/kubernetes/manifests/coredns.yml", 'data': coredns_data},
                              {'path': "/etc/kubernetes/Corefile.template", 'data': corefile_data},
                              {"path": "/etc/NetworkManager/dispatcher.d/99-forcedns", "data": forcedns_data,
                               "mode": int('755', 8)}])
        if api_ip is not None:
            data['virtual_router_id'] = data.get('virtual_router_id') or hash(cluster) % 254 + 1
            virtual_router_id = data['virtual_router_id']
            pprint(f"Using keepalived virtual_router_id {virtual_router_id}")
            data['auth_pass'] = ''.join(choice(ascii_letters + digits) for i in range(5))
            vips = [api_ip, ingress_ip] if ingress_ip is not None else [api_ip]
            pprint("Injecting keepalived static pod with %s" % ','.join(vips))
            keepalived_data = config.process_inputfile(cluster, f"{plandir}/staticpods/keepalived.yml", overrides=data)
            keepalivedconf_data = config.process_inputfile(cluster, f"{plandir}/keepalived.conf", overrides=data)
            sno_files.extend([{"path": "/etc/kubernetes/manifests/keepalived.yml", "data": keepalived_data},
                              {"path": "/etc/kubernetes/keepalived.conf.template", "data": keepalivedconf_data}])
        if sno_cpuset is not None:
            pprint("Injecting workload partitioning files")
            partitioning_data = config.process_inputfile(cluster, f"{plandir}/01-workload-partitioning", overrides=data)
            pinning_data = config.process_inputfile(cluster, f"{plandir}/openshift-workload-pinning", overrides=data)
            sno_files.extend([{"path": "/etc/crio/crio.conf.d/01-workload-partitioning", "data": partitioning_data},
                              {"path": "/etc/kubernetes/openshift-workload-pinning", "data": pinning_data}])
        if sno_relocate:
            pprint("Enabling relocation")
            relocate_script_data = config.process_inputfile(cluster, f"{plandir}/relocate-ip.sh", overrides=data)
            sno_files.append({"path": "/usr/local/bin/relocate-ip.sh", "mode": 448, "data": relocate_script_data})
        if sno_files:
            rendered = config.process_inputfile(cluster, f"{plandir}/99-sno.yaml", overrides={'files': sno_files,
                                                                                              'relocate': sno_relocate})
            with open(f"{clusterdir}/openshift/99-sno.yaml", 'w') as f:
                f.write(rendered)
        if sno_localhost_fix:
            localctlplane = config.process_inputfile(cluster, f"{plandir}/99-localhost-fix.yaml",
                                                     overrides={'role': 'master'})
            with open(f"{clusterdir}/openshift/99-localhost-fix-ctlplane.yaml", 'w') as _f:
                _f.write(localctlplane)
            localworker = config.process_inputfile(cluster, f"{plandir}/99-localhost-fix.yaml",
                                                   overrides={'role': 'worker'})
            with open(f"{clusterdir}/openshift/99-localhost-fix-worker.yaml", 'w') as _f:
                _f.write(localworker)
        if sno_ctlplanes:
            ingress = config.process_inputfile(cluster, f"{plandir}/customisation/99-ingress-controller.yaml",
                                               overrides={'role': 'master', 'cluster': cluster, 'domain': domain,
                                                          'replicas': 3})
            with open(f"{clusterdir}/openshift/99-ingress-controller.yaml", 'w') as _f:
                _f.write(ingress)
        pprint("Generating bootstrap-in-place ignition")
        run = call(f'openshift-install --dir={clusterdir} --log-level={log_level} create single-node-ignition-config',
                   shell=True)
        if run != 0:
            msg = "Hit issue when generating bootstrap-in-place ignition"
            return {'result': 'failure', 'reason': msg}
        move(f"{clusterdir}/bootstrap-in-place-for-live-iso.ign", f"./{sno_name}.ign")
        with open("iso.ign", 'w') as f:
            iso_overrides = {'image': 'rhcos4000'}
            extra_args = overrides.get('extra_args')
            _files = [{"path": "/root/sno-finish.service", "origin": f"{plandir}/sno-finish.service"},
                      {"path": "/usr/local/bin/sno-finish.sh", "origin": f"{plandir}/sno-finish.sh", "mode": 700}]
            if notify:
                _files.append({"path": "/root/kubeconfig", "origin": f'{clusterdir}/auth/kubeconfig'})
            if ipv6 or sno_disable_nics:
                nm_data = config.process_inputfile(cluster, f"{plandir}/kcli-ipv6.conf.j2", overrides=data)
                _files.append({'path': "/etc/NetworkManager/conf.d/kcli-ipv6.conf", 'content': nm_data})
            if sno_relocate:
                relocate_script_data = config.process_inputfile(cluster, f"{plandir}/relocate-ip-bootstrap.sh",
                                                                overrides=data)
                _files.append({"path": "/usr/local/bin/relocate-ip.sh", "mode": 700, "content": relocate_script_data})
                _files.append({"path": "/root/relocate-ip.service",
                               "origin": f"{plandir}/relocate-ip-bootstrap.service"})
            iso_overrides['files'] = _files
            iso_overrides.update(data)
            result = config.create_vm(sno_name, overrides=iso_overrides, onlyassets=True)
            pprint("Writing iso.ign to current dir")
            f.write(result['userdata'])
        if config.type == 'fake':
            pprint("Storing generated iso in current dir")
            generate_rhcos_iso(k, f"{cluster}-sno", 'default', installer=True, extra_args=extra_args)
        else:
            iso_pool = data['pool'] or config.pool
            pprint(f"Storing generated iso in pool {iso_pool}")
            generate_rhcos_iso(k, f"{cluster}-sno", iso_pool, installer=True, extra_args=extra_args)
        if sno_ctlplanes:
            if api_ip is None:
                warning("sno ctlplanes requires api vip to be defined. Skipping")
            else:
                ctlplane_overrides = overrides.copy()
                ctlplane_overrides['role'] = 'master'
                ctlplane_overrides['image'] = 'rhcos410'
                config.create_openshift_iso(cluster, overrides=ctlplane_overrides, installer=True)
        if sno_workers:
            worker_overrides = overrides.copy()
            worker_overrides['role'] = 'worker'
            worker_overrides['image'] = 'rhcos410'
            config.create_openshift_iso(cluster, overrides=worker_overrides, installer=True)
        if ignore_hosts:
            warning("Not updating /etc/hosts as per your request")
        elif api_ip is not None:
            update_openshift_etc_hosts(cluster, domain, api_ip)
        elif sno_dns:
            warning("Add the following entry in /etc/hosts if needed")
            dnsentries = ['api', 'console-openshift-console.apps', 'oauth-openshift.apps',
                          'prometheus-k8s-openshift-monitoring.apps']
            dnsentry = ' '.join([f"{entry}.{cluster}.{domain}" for entry in dnsentries])
            warning(f"$your_node_ip {dnsentry}")
        if baremetal_hosts:
            iso_pool = data['pool'] or config.pool
            iso_url = handle_baremetal_iso_sno(config, plandir, cluster, data, baremetal_hosts, iso_pool)
            result = boot_baremetal_hosts(baremetal_hosts, iso_url, overrides=overrides, debug=config.debug)
            if result['result'] != 'success':
                return result
        if sno_wait:
            installcommand = f'openshift-install --dir={clusterdir} --log-level={log_level} wait-for install-complete'
            installcommand = ' || '.join([installcommand for x in range(retries)])
            pprint("Launching install-complete step. It will be retried extra times in case of timeouts")
            run = call(installcommand, shell=True)
            if run != 0:
                msg = "Leaving environment for debugging purposes. "
                msg += f"Delete it with kcli delete cluster --yes {cluster}"
                return {'result': 'failure', 'reason': msg}
        else:
            c = os.environ['KUBECONFIG']
            kubepassword = open(f"{clusterdir}/auth/kubeadmin-password").read()
            console = f"https://console-openshift-console.apps.{cluster}.{domain}"
            info2(f"To access the cluster as the system:admin user when running 'oc', run export KUBECONFIG={c}")
            info2(f"Access the Openshift web-console here: {console}")
            info2(f"Login to the console with user: kubeadmin, password: {kubepassword}")
            if not baremetal_hosts:
                pprint(f"Plug {cluster}-sno.iso to your SNO node to complete the installation")
            if sno_ctlplanes:
                pprint(f"Plug {cluster}-master.iso to get additional ctlplanes")
            if sno_workers:
                pprint(f"Plug {cluster}-worker.iso to get additional workers")
        backup_paramfile(installparam, clusterdir, cluster, plan, image, dnsconfig)
        os.environ['KUBECONFIG'] = f"{clusterdir}/auth/kubeconfig"
        if sno_wait:
            process_apps(config, clusterdir, apps, overrides)
        return {'result': 'success'}
    if autoscale:
        commondir = os.path.dirname(pprint.__code__.co_filename)
        autoscale_overrides = {'cluster': cluster, 'kubetype': 'openshift', 'workers': workers, 'replicas': 1}
        autoscale_data = config.process_inputfile(cluster, f"{commondir}/autoscale.yaml.j2",
                                                  overrides=autoscale_overrides)
        with open(f"{clusterdir}/openshift/99-autoscale.yaml", 'w') as f:
            f.write(autoscale_data)
    run = call(f'openshift-install --dir={clusterdir} --log-level={log_level} create ignition-configs', shell=True)
    if run != 0:
        msg = "Hit issues when generating ignition-config files"
        msg += ". Leaving environment for debugging purposes, "
        msg += f"Delete it with kcli delete kube --yes {cluster}"
        return {'result': 'failure', 'reason': msg}
    if platform in virtplatforms:
        overrides['virtual_router_id'] = data.get('virtual_router_id') or hash(cluster) % 254 + 1
        virtual_router_id = overrides['virtual_router_id']
        pprint(f"Using keepalived virtual_router_id {virtual_router_id}")
        installparam['virtual_router_id'] = virtual_router_id
        auth_pass = ''.join(choice(ascii_letters + digits) for i in range(5))
        overrides['auth_pass'] = auth_pass
        installparam['auth_pass'] = auth_pass
        pprint(f"Using {api_ip} for api vip....")
        host_ip = api_ip if platform != "openstack" or provider_network else public_api_ip
        if ignore_hosts or (not kubevirt_ignore_node_port and kubevirt_api_service and kubevirt_api_service_node_port):
            warning("Ignoring /etc/hosts")
        else:
            update_openshift_etc_hosts(cluster, domain, host_ip, ingress_ip)
    bucket_url = None
    if platform in cloudplatforms + ['openstack']:
        bucket = "%s-%s" % (cluster, domain.replace('.', '-'))
        if bucket not in config.k.list_buckets():
            config.k.create_bucket(bucket)
        config.k.upload_to_bucket(bucket, f"{clusterdir}/bootstrap.ign", public=True)
        bucket_url = config.k.public_bucketfile_url(bucket, "bootstrap.ign")
    move(f"{clusterdir}/master.ign", f"{clusterdir}/master.ign.ori")
    move(f"{clusterdir}/worker.ign", f"{clusterdir}/worker.ign.ori")
    with open(f"{clusterdir}/worker.ign.ori") as f:
        ignition_version = json.load(f)['ignition']['version']
        installparam['ignition_version'] = ignition_version
    create_ignition_files(config, plandir, cluster, domain, api_ip=api_ip, bucket_url=bucket_url,
                          ignition_version=ignition_version)
    backup_paramfile(installparam, clusterdir, cluster, plan, image, dnsconfig)
    if platform in virtplatforms:
        if platform == 'vsphere':
            pprint(f"Creating vm folder /vm/{cluster}")
            k.create_vm_folder(cluster)
        pprint("Deploying bootstrap")
        result = config.plan(plan, inputfile=f'{plandir}/bootstrap.yml', overrides=overrides)
        if result['result'] != 'success':
            return result
        if static_networking_ctlplane:
            wait_for_ignition(cluster, domain, role='master')
        pprint("Deploying ctlplanes")
        threaded = data.get('threaded', False) or data.get('ctlplanes_threaded', False)
        if baremetal_hosts:
            overrides['workers'] = overrides['workers'] - len(baremetal_hosts)
        result = config.plan(plan, inputfile=f'{plandir}/ctlplanes.yml', overrides=overrides, threaded=threaded)
        if result['result'] != 'success':
            return result
        if dnsconfig is not None:
            dns_overrides = {'api_ip': api_ip, 'ingress_ip': ingress_ip, 'cluster': cluster, 'domain': domain}
            result = dnsconfig.plan(plan, inputfile=f'{plandir}/cloud_dns.yml', overrides=dns_overrides)
            if result['result'] != 'success':
                return result
    else:
        pprint("Deploying bootstrap")
        result = config.plan(plan, inputfile=f'{plandir}/cloud_bootstrap.yml', overrides=overrides)
        if result['result'] != 'success':
            return result
        if platform == 'ibm':
            while api_ip is None:
                api_ip = k.info(f"{cluster}-bootstrap").get('private_ip')
                pprint("Gathering bootstrap private ip")
                sleep(10)
            sedcmd = f'sed -i "s@api-int.{cluster}.{domain}@{api_ip}@" {clusterdir}/ctlplane.ign'
            call(sedcmd, shell=True)
        pprint("Deploying ctlplanes")
        threaded = data.get('threaded', False) or data.get('ctlplanes_threaded', False)
        result = config.plan(plan, inputfile=f'{plandir}/cloud_ctlplanes.yml', overrides=overrides, threaded=threaded)
        if result['result'] != 'success':
            return result
        if platform == 'ibm':
            first_ctlplane_ip = None
            while first_ctlplane_ip is None:
                first_ctlplane_ip = k.info(f"{cluster}-ctlplane-0").get('private_ip')
                pprint("Gathering first ctlplane bootstrap ip")
                sleep(10)
            sedcmd = f'sed -i "s@api-int.{cluster}.{domain}@{first_ctlplane_ip}@" {clusterdir}/worker.ign'
            call(sedcmd, shell=True)
        result = config.plan(plan, inputfile=f'{plandir}/cloud_lb_api.yml', overrides=overrides)
        if result['result'] != 'success':
            return result
        if workers == 0:
            lb_overrides = {'cluster': cluster, 'domain': domain, 'members': ctlplanes}
            if 'dnsclient' in overrides:
                lb_overrides['dnsclient'] = overrides['dnsclient']
            result = config.plan(plan, inputfile=f'{plandir}/cloud_lb_apps.yml', overrides=lb_overrides)
            if result['result'] != 'success':
                return result
    if not kubevirt_ignore_node_port and kubevirt_api_service and kubevirt_api_service_node_port:
        nodeport = k.get_node_ports(f'{cluster}-api', k.namespace)[6443]
        sedcmd = f'sed -i "s@:6443@:{nodeport}@" {clusterdir}/auth/kubeconfig'
        call(sedcmd, shell=True)
        while True:
            nodehost = k.info(f"{cluster}-bootstrap").get('host')
            if nodehost is not None:
                break
            else:
                pprint("Waiting 5s for bootstrap vm to be up")
                sleep(5)
        if 'KUBECONFIG' in os.environ or 'kubeconfig' in config.ini[config.client]:
            kubeconfig = config.ini[config.client].get('kubeconfig') or os.environ['KUBECONFIG']
            hostip_cmd = f'KUBECONFIG={kubeconfig} oc get node {nodehost} -o yaml'
            hostip = yaml.safe_load(os.popen(hostip_cmd).read())['status']['addresses'][0]['address']
            update_openshift_etc_hosts(cluster, domain, hostip)
    if not async_install:
        bootstrapcommand = f'openshift-install --dir={clusterdir} --log-level={log_level} wait-for bootstrap-complete'
        bootstrapcommand = ' || '.join([bootstrapcommand for x in range(retries)])
        run = call(bootstrapcommand, shell=True)
        if run != 0:
            msg = "Leaving environment for debugging purposes. "
            msg += f"Delete it with kcli delete cluster --yes {cluster}"
            return {'result': 'failure', 'reason': msg}
        if dnsconfig is not None:
            pprint(f"Deleting Dns entry for {cluster}-bootstrap in {domain}")
            z = dnsconfig.k
            z.delete_dns(f"{cluster}-bootstrap", domain)
        delete_lastvm(f"{cluster}-bootstrap", config.client)
    if workers > 0:
        if static_networking_worker:
            wait_for_ignition(cluster, domain, role='worker')
        pprint("Deploying workers")
        if 'name' in overrides:
            del overrides['name']
        if platform in virtplatforms:
            if baremetal_hosts:
                iso_pool = data.get('pool') or config.pool
                iso_url = handle_baremetal_iso(config, plandir, cluster, data, baremetal_hosts, iso_pool)
                result = boot_baremetal_hosts(baremetal_hosts, iso_url, overrides=overrides, debug=config.debug)
                if result['result'] != 'success':
                    return result
            if overrides['workers'] > 0:
                threaded = data.get('threaded', False) or data.get('workers_threaded', False)
                result = config.plan(plan, inputfile=f'{plandir}/workers.yml', overrides=overrides, threaded=threaded)
                if result['result'] != 'success':
                    return result
        elif platform in cloudplatforms:
            result = config.plan(plan, inputfile=f'{plandir}/cloud_workers.yml', overrides=overrides)
            if result['result'] != 'success':
                return result
            lb_overrides = {'cluster': cluster, 'domain': domain, 'members': workers, 'role': 'worker'}
            result = config.plan(plan, inputfile=f'{plandir}/cloud_lb_apps.yml', overrides=lb_overrides)
            if result['result'] != 'success':
                return result
    if async_install:
        kubeconf = os.environ['KUBECONFIG']
        kubepassword = open(f"{clusterdir}/auth/kubeadmin-password").read()
        if async_install:
            success("Async Cluster created")
            info2("You will need to wait before it is fully available")
        info2(f"To access the cluster as the system:admin user when running 'oc', run export KUBECONFIG={kubeconf}")
        info2(f"Access the Openshift web-console here: https://console-openshift-console.apps.{cluster}.{domain}")
        info2(f"Login to the console with user: kubeadmin, password: {kubepassword}")
        return {'result': 'success'}
    else:
        installcommand = f'openshift-install --dir={clusterdir} --log-level={log_level} wait-for install-complete'
        installcommand += f" || {installcommand}"
        pprint("Launching install-complete step. It will be retried one extra time in case of timeouts")
        run = call(installcommand, shell=True)
        if run != 0:
            msg = "Leaving environment for debugging purposes. "
            msg += f"Delete it with kcli delete cluster --yes {cluster}"
            return {'result': 'failure', 'reason': msg}
    pprint(f"Deleting {cluster}-bootstrap")
    k.delete(f"{cluster}-bootstrap")
    if platform in cloudplatforms:
        bucket = "%s-%s" % (cluster, domain.replace('.', '-'))
        config.k.delete_bucket(bucket)
    if original_domain is not None:
        overrides['domain'] = original_domain
    if config.type in cloudplatforms:
        wait_cloud_dns(cluster, domain)
    os.environ['KUBECONFIG'] = f"{clusterdir}/auth/kubeconfig"
    process_apps(config, clusterdir, apps, overrides)
    process_postscripts(clusterdir, postscripts)
    if platform in cloudplatforms and ctlplanes == 1 and workers == 0 and data.get('sno_cloud_remove_lb', True):
        pprint("Removing loadbalancers as there is a single ctlplane")
        k.delete_loadbalancer(f"api.{cluster}")
        k.delete_loadbalancer(f"apps.{cluster}")
        api_ip = k.info(f"{cluster}-ctlplane-0").get('ip')
        k.delete_dns(f'api.{cluster}', domain=domain)
        k.reserve_dns(f'api.{cluster}', domain=domain, ip=api_ip)
        k.delete_dns(f'apps.{cluster}', domain=domain)
        k.reserve_dns(f'apps.{cluster}', domain=domain, ip=api_ip, alias=['*'])
        if platform == 'ibm':
            k._add_sno_security_group(cluster)
    return {'result': 'success'}
